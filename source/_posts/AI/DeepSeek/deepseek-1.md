---
title: DeepSeek R1 架構與訓練過程 
date: 2025-02-09
tags: [DeepSeek, LLM, 強化學習]
categories: [AI]
---

大家好! 這篇文章會用最簡單的方式, 帶大家了解 DeepSeek R1 這個厲害的 AI 模型。

我們會用一個簡單的數學題 `2 + 3 * 4 等於多少?`, 來一步一步解釋 DeepSeek R1 的技術細節。

## 1、快速導覽

在深入了解技術之前, 讓我們先來看看 DeepSeek R1 是怎麼來的。它不是從零開始打造的, 而是基於一個已經很聰明的 AI 模型 DeepSeek V3。DeepSeek 團隊想要讓 V3 更會「推理」, 就像讓它變成解謎高手一樣!

他們使用了「強化學習」(Reinforcement Learning, 簡稱 RL) 這個方法。簡單來說, 就像訓練寵物一樣, 當 AI 做出對推理有幫助的事情時, 就給它獎勵 (例如加分); 做錯了, 就處罰它 (例如扣分)。

但這不是普通的訓練, 而是一整套流程, 就像一個精心設計的課程。他們先試試看純粹用 RL, 看 AI 會不會自己學會推理, 這就是 DeepSeek R1 Zero, 像是一個實驗品。然後, 他們把真正的 DeepSeek R1 分成好幾個階段, 一步一步加強它。先給它一些資料學習, 再用 RL 訓練, 然後再給更多資料, 再用 RL 訓練... 就像打怪升級一樣!

總之, DeepSeek R1 的目標是讓 AI 模型更會思考, 給你更聰明的答案, 而不是只會講一堆沒意義的字。

所以, 在我們開始研究複雜的細節之前, 先給你一個簡單的概念。

## 2、DeepSeek V3 (MOE) 怎麼思考?

就像我們前面說的, DeepSeek R1 是基於 DeepSeek V3 這個模型。所以, 我們要先了解 V3 是怎麼運作的, 還有為什麼它叫做 MOE (混合專家模型)。

DeepSeek V3 有兩條主要的思考路徑, 就像人類在思考問題時, 有時候會直接反應, 有時候會仔細分析。當你給它一個問題, 它會先經過一個「記憶系統」, 這個系統會快速找到相關的資訊。你可以想像成, 它在快速回想以前有沒有遇過類似的狀況, 或是從書本、網路等地方搜尋相關資料。

V3 最厲害的地方是它的「決策系統」。當它理解了你輸入的問題後, 它會用一個聰明的「路由器」來決定要走哪一條路徑:

*   **快速通道**: 用來處理簡單的問題, 像是簡單的計算 (例如 1 + 1 = ?) 或是常見的要求 (例如「今天天氣如何?」)。
*   **專家系統**: 用來處理複雜的問題, 像是分析 (例如分析股票走勢) 或是需要專業知識的問題 (例如「如何治療感冒?」)。

這個「路由器」讓 DeepSeek V3 變成一個「混合專家模型」(MOE), 因為它可以根據不同的問題, 選擇最適合的「專家」來處理。你可以想像成, 它有一個團隊, 裡面有數學專家、天氣專家、醫學專家等等, 遇到不同的問題, 就找不同的專家來幫忙。

簡單的問題可以快速得到答案, 複雜的問題也能得到詳細的分析。最後, 這些答案會組合成清楚又準確的結果。

**簡單來說, DeepSeek V3 就像一個擁有很多不同領域專家的團隊, 遇到問題時, 就會指派最適合的專家來解決。**

## 3、DeepSeek V3 變成 RL 訓練的基礎

現在我們了解 DeepSeek V3 怎麼思考了, 它是 DeepSeek R1 的起點。就像我們前面提到的, 他們先用 V3 創造了一個叫做 DeepSeek R1 Zero 的版本, 這個版本就像是正式版之前的測試版, 還有一些問題需要解決。

R1 Zero 是用「強化學習」訓練出來的, V3 就像是一個在遊戲中控制角色的人, 它會根據遊戲的狀況做出不同的行動。在這個例子中, 「遊戲」就是推理任務本身, 也就是讓 AI 模型解決各種問題。

V3 做出行動後, 「環境」會給它獎勵。這個獎勵就像是回饋, 告訴 V3 它的行動做得好不好。如果 V3 答對了問題或是推理得很棒, 就會得到正面的獎勵 (例如加分)。這個獎勵會幫助 V3 學習, 讓它知道下次要怎麼做才能得到更好的獎勵。

**你可以把強化學習想像成訓練小狗, 答對了就給牠獎勵, 答錯了就稍微處罰牠, 讓牠慢慢學會什麼是對的。**

接下來, 我們會更深入討論這個獎勵模型, 還有他們使用的 RL 演算法。

## 4、GRPO 演算法: 讓 AI 更有效率地學習

訓練 AI 模型需要花很多錢, 而「強化學習」又讓這件事變得更複雜。

雖然有很多 RL 演算法可以用, 但傳統的 RL 會用一個叫做「批評家」的東西來幫助主要的決策者 (也就是 DeepSeek V3)。這個「批評家」通常跟決策者一樣厲害, 這就讓訓練的成本變成兩倍。你可以想像成, 不但要訓練一個很會解題的學生, 還要訓練一個很會批評的老師, 成本當然很高。

但 GRPO 的方法不一樣, 它可以直接從 AI 的行動結果中找到一個基準點, 也就是一個好的行動應該是什麼樣子。所以, GRPO 不需要額外的「批評家」模型, 這省下了很多的計算資源, 讓訓練變得更有效率。

GRPO 會先給 AI 模型一個問題, 然後讓它用不同的方法來回答。接著, GRPO 會評估每一個答案, 給它們一個分數, 來表示這個答案的好壞。

GRPO 會把每一個答案跟其他答案的平均分數比較, 算出這個答案的「優勢」。如果答案比平均好, 就會得到正面的優勢 (代表這個答案很棒); 如果答案比平均差, 就會得到負面的優勢 (代表這個答案不太好)。重點是, 這個過程不需要額外的「批評家」模型。

最後, GRPO 會用這些「優勢」分數來調整 AI 模型的策略, 讓它更有可能在未來產生更好的答案。這個調整過後的模型會變成新的基礎, 然後重複這個過程, 不斷地改進 AI 模型。

**GRPO 就像是一個聰明的教練, 它可以根據 AI 模型的表現, 找出它做得好的地方和需要改進的地方, 然後給它一些建議, 讓它變得更厲害。**

## 5、GRPO 的目標: 讓 AI 穩定地進步

GRPO 背後有一些複雜的數學公式, 但簡單來說, GRPO 的目標有兩個:

1.  給出好的答案 (得到高分)。
2.  確保訓練過程穩定, 不會出錯。

GRPO 會用一些方法來確保 AI 模型不會一下子改變太多, 這樣可以避免 AI 模型學到錯誤的東西。就像開車一樣, 如果方向盤一下子轉太大力, 很容易發生意外。

**GRPO 就像是一個安全機制, 它可以防止 AI 模型在學習的過程中走偏, 確保它能夠穩定地進步。**

## 6、DeepSeek R1 Zero 的獎勵機制

現在我們了解了一些基本的概念, 讓我們來看看 DeepSeek 團隊是怎麼訓練 R1 Zero 的。

他們用一個很簡單的方法來給 AI 模型獎勵, 也就是「基於規則的獎勵系統」。

舉例來說, 如果我們問 AI 模型:「2 + 3 * 4 等於多少?」

### 6.1 檢查答案

系統知道正確答案是 14。它會檢查 AI 模型給出的答案, 看看裡面有沒有包含「14」這個數字。

*   如果答案是 14, AI 模型就會得到獎勵 (例如 +1 分)。
*   如果答案錯了, AI 模型就得不到獎勵 (0 分), 甚至可能會被扣分。

### 6.2 檢查格式

除了答案對不對, DeepSeek R1 Zero 還要學習怎麼正確地表達它的推理過程。它需要學會用 `` 和 `` 標籤, 把推理過程和答案分開。

如果 AI 模型正確地使用了這些標籤, 它也會得到一些獎勵。

**這個獎勵機制就像考試一樣, 答對了有分, 格式正確也有分, 鼓勵 AI 模型不只要求準確, 還要表達清楚。**

## 7、獎勵訓練的模板

為了讓獎勵機制更有效, DeepSeek 團隊設計了一個特定的「訓練模板」。這個模板就像是一個範本, 告訴 AI 模型在訓練的時候應該怎麼組織它的回答。

這個模板長得像這樣:

```
用戶和助手之間的對話。用戶提出問題,助手解決該問題。助手首先在腦海中思考推理過程,然後為用戶提供答案。推理過程和答案分別包含在 <think></think> 和 <answer></answer> 標籤中,即 <think>推理過程在這裡</think> <answer>答案在這裡</answer> 。用戶:{prompt}。助手:
```

在這個模板裡面,`{prompt}` 會被替換成實際的問題,例如「2 + 3 * 4 等於多少?」。

AI 模型需要根據這個模板來產生它的回答,像是這樣:

```
<think>
運算順序:先乘後加。3 * 4 = 12。2 + 12 = 14
</think>
<answer>
14
</answer>
```

DeepSeek 團隊故意讓這個模板很簡單,只專注在結構上,而不是告訴 AI 模型要怎麼推理。

**這個模板就像是一個作文的格式, 讓 AI 模型知道回答問題的時候, 應該包含哪些部分, 並且按照一定的順序來呈現。**

## 8、DeepSeek R1 Zero 的訓練過程

DeepSeek 團隊先用 DeepSeek V3 這個模型, 產生很多可能的答案。然後, 他們會根據答案的正確性和推理品質來給這些答案評分。

為了讓 AI 模型學會更好的推理, 他們設計了一個「基於規則的獎勵系統」。這個系統會根據以下條件來給答案獎勵:

*   **準確度獎勵**: 答案是否正確。
*   **格式獎勵**: 推理步驟是否用 `` 標籤正確地格式化。

AI 模型會學會根據這些獎勵來調整它的行為。如果它給出正確的答案, 並且用正確的格式來表達它的推理過程, 它就會得到比較高的分數。反之, 如果它給出錯誤的答案, 或是沒有用正確的格式, 它就會得到比較低的分數。

就像我們訓練小狗一樣, 當牠做對了, 我們就給牠零食; 當牠做錯了, 我們就輕輕地責備牠。透過這種方式, AI 模型會慢慢地學會怎麼做才是最好的。

## 9、R1 Zero 有什麼問題?

雖然 DeepSeek R1 Zero 在某些方面表現得很好, 但它還是有一些問題。

其中一個問題是, 它產生的推理過程有時候很難理解。就像是小朋友寫的作文一樣, 雖然內容是對的, 但是表達方式不夠清楚, 大人很難看懂。

另外一個問題是, 它有時候會搞錯語言。如果我們用西班牙文問它問題, 它可能會用英文和西班牙文混雜的方式來回答, 讓人覺得很奇怪。

**這就像是一個翻譯軟體, 雖然可以把文字翻譯出來, 但是翻譯的品質不夠好, 讓人很難理解。**

這些問題讓 DeepSeek 團隊決定要繼續改進 R1 Zero, 創造出更厲害的 DeepSeek R1。

## 10、冷啟動資料: 讓 AI 學習正確的推理方式

為了讓 DeepSeek R1 變得更聰明, DeepSeek 團隊做了一件很重要的事, 就是收集「冷啟動資料」。

你可以把「冷啟動資料」想像成是 AI 模型的教科書, 裡面有很多正確的推理例子, 讓 AI 模型可以學習。就像小朋友剛開始學寫字的時候, 老師會給他們看一些範本, 讓他們模仿。

### 10.1 用大量的例子來引導 AI

DeepSeek 團隊給 DeepSeek V3 看了一些問題, 還有非常詳細的解題步驟。這個方法叫做「思維鏈」(Chain of Thought, 簡稱 CoT)。就像是老師教學生解數學題一樣, 一步一步地講解, 讓學生知道為什麼要這樣做。

舉例來說, 如果問題是「2 + 3 * 4 等於多少?」, 他們可能會給 AI 模型看以下的例子:

```
問題: 9 的平方根加上 5 是多少?
答案: | special_token | 首先, 算出 9 的平方根, 答案是 3。然後, 把 3 加上 5。3 + 5 等於 8。 | special_token | 總結: 答案是 8。

問題: 火車以時速 60 英里行駛 2 小時, 總共走了多遠?
答案: | special_token | 使用公式: 距離 = 速度 x 時間。速度是 60 英里/小時, 時間是 2 小時。距離 = 60 x 2 = 120 英里。 | special_token | 總結: 答案是 120 英里。

問題: 2 + 3 * 4 等於多少?
答案:
```

`| special_token |` 就像是一個分隔符號, 把推理步驟和總結分開, 讓 AI 模型可以清楚地學習。

看完這些例子後, AI 模型應該學會用類似的格式來回答問題, 像是這樣:

```
| special_token | 根據運算順序 (先乘除後加減), 先算 3 * 4 = 12。然後, 把 2 加上 12。2 + 12 = 14。 | special_token | 總結: 答案是 14。
```

### 10.2 直接要求 AI 展現推理過程

除了給 AI 模型看例子, DeepSeek 團隊還會直接要求 AI 模型不只回答問題,還要一步一步地展示它的推理過程, 並且驗證答案。

這樣可以讓 AI 模型更仔細、更周到地解決問題, 就像考試的時候, 老師會要求學生寫出計算過程, 而不是只寫答案。

舉例來說, 對於「2 + 3 * 4 等於多少?」這個問題, 他們可能會這樣問:

```
問題: 請解出這個問題, 一步一步展示你的推理過程, 並且驗證答案: 2 + 3 * 4 等於多少?
```

AI 模型需要給出包含推理步驟和驗證部分的答案, 像是這樣:

```
| special_token | 推理過程: 為了算出 2 + 3 * 4, 我需要使用運算順序。先乘除後加減。
步驟 1: 算出 3 * 4 = 12。
步驟 2: 把 2 加上步驟 1 的結果: 2 + 12 = 14。
驗證: 再次檢查運算順序, 確認先乘除後加減。計算過程看起來沒錯。
| special_token | 總結: 答案是 14。
```

### 10.3 改善 R1 Zero 的答案

DeepSeek 團隊甚至使用了已經訓練好的 R1 Zero 模型的答案。雖然 R1 Zero 還有一些問題, 但它已經可以做一些推理了。所以, 他們把 R1 Zero 的答案拿出來, 請人類專家把這些答案改得更清楚、更有結構, 並且修正任何錯誤。

舉例來說, R1 Zero 產生的答案可能是這樣:

`` ummm... 3 乘以 4... 得到 12... 然後加上 2...``
 14

人類專家會把它改成這樣:

```
| special_token | 推理過程: 為了算出這個答案, 我們需要使用運算順序, 先乘除後加減。
步驟 1: 3 乘以 4, 答案是 12。
步驟 2: 把 2 加上步驟 1 的結果: 2 + 12 = 14。
| special_token | 總結: 答案是 14。
```

透過這些方法, DeepSeek 團隊收集到了非常好的「冷啟動資料」。這些資料就像是高品質的教材, 可以幫助 AI 模型學到正確的推理方式。

## 11、監督式微調: 讓 AI 模仿好的推理方式

有了「冷啟動資料」後, DeepSeek 團隊就開始使用「監督式微調」(Supervised Fine-Tuning, 簡稱 SFT) 這個方法來訓練 AI 模型。

SFT 就像是讓 AI 模型看著教科書來學習。他們把「冷啟動資料」裡面的例子拿出來, 告訴 AI 模型:「當你看到這個問題的時候, 就要像這個例子一樣來回答。」

透過這種方式, AI 模型會慢慢地學會怎麼產生高品質、有結構的推理答案。就像學生模仿老師的解題步驟一樣, AI 模型會學會模仿「冷啟動資料」裡面的推理方式。

**你可以把「監督式微調」想像成是讓 AI 模型參加一個模仿比賽, 讓它模仿人類專家解題的過程, 並且給它評分, 讓它知道什麼是好的推理方式。**

## 12、推理導向強化學習: 讓 AI 更上一層樓

雖然「監督式微調」可以讓 AI 模型學會基本的推理方式, 但 DeepSeek 團隊還想要讓它變得更厲害。所以, 他們使用了「推理導向強化學習」這個方法。

在這個階段, 他們會用一些特別的技巧來訓練 AI 模型, 讓它更擅長推理。

其中一個技巧是「語言一致性獎勵」。就像我們前面提到的, R1 Zero 有時候會搞錯語言。為了避免這個問題, DeepSeek 團隊會給 AI 模型一些獎勵, 鼓勵它在回答問題的時候使用相同的語言。

舉例來說, 如果我們用英文問 AI 模型問題, 他們就會希望 AI 模型也用英文來回答, 包括它的推理過程和最終答案。這樣可以確保 AI 模型在思考和表達的時候, 都使用同一種語言, 避免產生混亂。

**「語言一致性獎勵」就像是一個翻譯檢查員, 它可以確保 AI 模型在回答問題的時候, 使用的語言是正確的, 避免產生翻譯錯誤。**

## 13、拒絕抽樣: 選擇最好的答案來訓練 AI

為了讓 AI 模型學到最好的推理方式, DeepSeek 團隊使用了一種叫做「拒絕抽樣」的技術。

你可以把這個技術想像成是一個篩選的過程。他們會讓 AI 模型產生很多不同的答案, 然後仔細檢查每一個答案, 只留下最好的答案來訓練 AI 模型。

就像是選秀節目一樣, 只有通過評審的參賽者才能進入下一輪比賽。透過這種方式, DeepSeek 團隊可以確保 AI 模型學到的都是高品質的推理例子。

**「拒絕抽樣」就像是一個嚴格的品質管制員, 它可以確保 AI 模型學到的都是最好的答案, 避免學到錯誤的或是不好的推理方式。**

## 14、適用於所有場景的強化學習: 讓 AI 更安全、更有用

在經過前面的訓練後, DeepSeek R1 已經變得非常聰明了。但是, DeepSeek 團隊還想要讓它變得更安全、更有用。所以, 他們使用了「適用於所有場景的強化學習」這個方法。

在這個階段, 他們會訓練 AI 模型考慮到人類的價值觀。舉例來說, 他們會告訴 AI 模型:「在回答問題的時候, 要盡量給出有用的資訊, 並且避免產生任何有害的內容。」

就像是教導小朋友一樣, 讓他們知道什麼是對的, 什麼是錯的。透過這種方式, DeepSeek R1 可以成為一個更值得信賴的 AI 助手。

**「適用於所有場景的強化學習」就像是一個道德指南針, 它可以引導 AI 模型在回答問題的時候, 考慮到人類的價值觀, 避免產生任何有害的內容。**

## 15、蒸餾: 讓更多人可以使用 DeepSeek R1

DeepSeek 團隊成功打造出 DeepSeek R1 這個厲害的 AI 模型後, 他們還做了一件很棒的事情, 就是把這個模型「蒸餾」成更小的版本。

你可以把「蒸餾」想像成是把大鍋飯濃縮成小包裝。透過這種方式, 更多人可以使用 DeepSeek R1 的技術, 即使他們的電腦沒有那麼厲害。

**「蒸餾」就像是一個壓縮技術, 它可以把 AI 模型變得更小、更快, 讓更多人可以使用它。**

總結來說, DeepSeek R1 的訓練過程就像是一個精心設計的課程, 透過不同的方法和技巧, 讓 AI 模型一步一步地學會推理, 並且變得更安全、更有用。希望這篇文章可以幫助你更了解 DeepSeek R1 這個厲害的 AI 模型!

**希望這篇文章能夠幫助你了解 DeepSeek R1 的訓練過程, 讓你對 AI 的發展有更深入的認識!**

#參考
https://levelup.gitconnected.com/drawing-deepseek-r1-architecture-and-training-process-from-scratch-72043da33955